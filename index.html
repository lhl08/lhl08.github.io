<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Tony Haolin Li</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/x-icon" sizes="32x32" href="favicon-32x32.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<body>
<div class="page-shell">

  <!-- Hero Section -->
  <div class="hero-row">
    <div class="hero-text">
      <p class="name">Tony Haolin Li</p>
      <p>I am a second-year Ph.D. student in the <a href="https://www.cs.stonybrook.edu/">Computer Science Department</a> of <a href="https://www.stonybrook.edu/">Stony Brook University</a>, where I am fortunate enough to be supervised by <a href="https://www3.cs.stonybrook.edu/~xiaojun/">Prof. Xiaojun Bi</a>, as a member of <a href="http://hail.cs.stonybrook.edu/">Human-AI Interaction Lab</a> (HAIL).</p>
      <p>I received my B.E. in Computer Science and Technology from <a href="http://en.ustc.edu.cn">University of Science and Technology of China</a> (USTC) in 2024. During my undergraduate studies, I worked closely with <a href="https://www.cs.dartmouth.edu/~xingdong/">Prof. Xing-Dong Yang</a> and <a href="https://teyenwu.com/">Prof. Te-Yen Wu</a>.</p>
      <p>I am deeply passionate about AI-driven <b>Human-Computer Interaction</b>, with a particular focus on <b>input</b> systems (text entry, automatic GUI agents, etc.) powered by <b>Large Language Models</b> and <b>Machine Learning</b>.</p>
      <div class="social-links">
        <a href="mailto:haolili@cs.stonybrook.edu">Email</a>
        <a href="https://scholar.google.com/citations?hl=en&user=FMNv2bAAAAAJ">Scholar</a>
        <a href="https://github.com/lhl08">GitHub</a>
        <a href="https://www.linkedin.com/in/tony-haolin-li-8622b027b/">LinkedIn</a>
        <a href="https://x.com/TonyH_Li">X</a>
        <a href="https://lhl08.github.io/Tony_Li_CV.pdf">CV</a>
      </div>
    </div>
    <div class="hero-photo">
      <img alt="Tony Haolin Li" src="images/tony.png">
    </div>
  </div>

  <!-- Research Section -->
  <div class="research-section">
    <h2>Research</h2>
  </div>

  <!-- Paper Cards -->
  <div class="paper-card">
    <div class="paper-thumb">
      <img src="images/keysense.png" alt="KeySense">
    </div>
    <div class="paper-body">
      <span class="papertitle"><a href="https://lhl08.github.io">KeySense: LLM-Powered Hands-Down, Ten-Finger Typing on Commodity Touchscreens</a></span>
      <div class="paper-meta">
        <b>Tony Li</b>, Yan Ma, Zhuojun Li, Chun Yu, IV Ramakrishnan, Xiaojun Bi
        <br>
        <em>ACM Conference on Human Factors in Computing Systems (CHI)</em>, 2026
      </div>
      <p>KeySense turns commodity touchscreens into hands-down ten-finger keyboards by distinguishing resting contacts from intentional keystrokes and decoding them with a fine-tuned large language model. It reuses physical-keyboard muscle memory, enabling fast, low-fatigue text entry without extra hardware.</p>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-thumb">
      <img src="images/CHI25BIT.png" alt="BIT">
    </div>
    <div class="paper-body">
      <span class="papertitle"><a href="https://www.youtube.com/watch?v=PuxLTwyZB2c">BIT: Battery-free, IC-less and Wireless Smart Textile Interface and Sensing System</a></span>
      <div class="paper-meta">
        Weiye Xu, <b>Tony Li</b>, Yuntao Wang, Xing-Dong Yang, Te-Yen Wu
        <br>
        <em>ACM Conference on Human Factors in Computing Systems (CHI)</em>, 2025
      </div>
      <p>BIT is a smart textile interface that eliminates batteries, ICs, and connectors, using multi-resonant circuits and wireless electromagnetic coupling for power and sensing. It supports resistive, capacitive, and inductive sensors, enabling seamless, sustainable, and flexible interactions in wearable technology.</p>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-thumb">
      <img src="images/vrdemo2.png" alt="VR Demo">
    </div>
    <div class="paper-body">
      <span class="papertitle"><a href="https://www.youtube.com/watch?v=YMORsSqYLtU">LLM-powered Text Entry in Virtual Reality</a></span>
      <div class="paper-meta">
        Yan Ma, <b>Tony Li</b>, Zhi Li, Xiaojun Bi
        <br>
        <em>IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)</em>, 2025, Research Demo
      </div>
      <p>This demo presents an LLM-powered VR text entry system that enhances input accuracy and flexibility. Supporting raycasting and joystick-based tap and word-gesture typing, it integrates a fine-tuned FLAN-T5 decoder, achieving 93.1% accuracy on word-gesture typing and 95.4% on tap typing.</p>
    </div>
  </div>

  <!-- Footer -->
  <p class="footer-note">
    Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the template.
  </p>
</div>

<div class="visitor-map">
  <script type="text/javascript" id="clustrmaps"
      src="//clustrmaps.com/map_v2.js?d=9SW5xQVh3EcOCo-URVA7Xm9ONMf0LMLmUvKq3hTXvck&cl=ffffff&w=240&h=150">
  </script>
</div>
</body>
</html>
